{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.layers import Flatten, Add\n",
    "from keras.layers import Concatenate, Activation\n",
    "from keras.layers import LeakyReLU, BatchNormalization, Lambda\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accw(y_true, y_pred):\n",
    "    y_pred=K.clip(y_pred, -1, 1)\n",
    "    return K.mean(K.equal(y_true, K.round(y_pred)))\n",
    "\n",
    "def mssim(y_true, y_pred):\n",
    "    costs = 1.0 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 2.0))\n",
    "    return costs\n",
    "\n",
    "def wloss(y_true,y_predict):\n",
    "    return -K.mean(y_true*y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inp_shape = (256,256,1), trainable = True):\n",
    "    gamma_init = RandomNormal(1., 0.02)\n",
    "    \n",
    "    inp = Input(shape = (256,256,1))\n",
    "    \n",
    "    l0 = Conv2D(64, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(inp) #b_init is set to none, maybe they are not using bias here, but I am.\n",
    "    l0 = LeakyReLU(alpha=0.2)(l0)\n",
    "    \n",
    "    l1 = Conv2D(64*2, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l0)\n",
    "    l1 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l1)\n",
    "    l1 = LeakyReLU(alpha=0.2)(l1)\n",
    "    \n",
    "    l2 = Conv2D(64*4, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l1)\n",
    "    l2 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l2)\n",
    "    l2 = LeakyReLU(alpha=0.2)(l2)\n",
    "    \n",
    "    l3 = Conv2D(64*8, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l2)\n",
    "    l3 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l3)\n",
    "    l3 = LeakyReLU(alpha=0.2)(l3)\n",
    "    \n",
    "    l4 = Conv2D(64*16, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l3)\n",
    "    l4 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l4)\n",
    "    l4 = LeakyReLU(alpha=0.2)(l4)\n",
    "    \n",
    "    l5 = Conv2D(64*32, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l4)\n",
    "    l5 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l5)\n",
    "    l5 = LeakyReLU(alpha=0.2)(l5)\n",
    "    \n",
    "    l6 = Conv2D(64*16, (1,1), strides = (1,1), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l5)\n",
    "    l6 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l6)\n",
    "    l6 = LeakyReLU(alpha=0.2)(l6)\n",
    "    \n",
    "    l7 = Conv2D(64*8, (1,1), strides = (1,1), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l6)\n",
    "    l7 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l7)\n",
    "    l7 = LeakyReLU(alpha=0.2)(l7)\n",
    "    #x\n",
    "    \n",
    "    l8 = Conv2D(64*2, (1,1), strides = (1,1), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l7)\n",
    "    l8 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l8)\n",
    "    l8 = LeakyReLU(alpha=0.2)(l8)\n",
    "    \n",
    "    l9 = Conv2D(64*2, (3,3), strides = (1,1), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l8)\n",
    "    l9 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l9)\n",
    "    l9 = LeakyReLU(alpha=0.2)(l9)\n",
    "    \n",
    "    l10 = Conv2D(64*8, (3,3), strides = (1,1), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l9)\n",
    "    l10 = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(l10)\n",
    "    l10 = LeakyReLU(alpha=0.2)(l10)\n",
    "    #y\n",
    "    l11 = Add()([l7,l10])\n",
    "    l11 = LeakyReLU(alpha = 0.2)(l11)\n",
    "    \n",
    "    out=Conv2D(filters=1,kernel_size=3,strides=1,padding='same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(l11)\n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resden(x,fil,gr,beta,gamma_init,trainable):    \n",
    "    x1=Conv2D(filters=gr,kernel_size=3,strides=1,padding='same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(x)\n",
    "    x1=BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(x1)\n",
    "    x1=LeakyReLU(alpha=0.2)(x1)\n",
    "    \n",
    "    x1=Concatenate(axis=-1)([x,x1])\n",
    "    \n",
    "    x2=Conv2D(filters=gr,kernel_size=3,strides=1,padding='same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(x1)\n",
    "    x2=BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(x2)\n",
    "    x2=LeakyReLU(alpha=0.2)(x2)\n",
    "\n",
    "    x2=Concatenate(axis=-1)([x1,x2])\n",
    "        \n",
    "    x3=Conv2D(filters=gr,kernel_size=3,strides=1,padding='same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(x2)\n",
    "    x3=BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(x3)\n",
    "    x3=LeakyReLU(alpha=0.2)(x3)\n",
    "\n",
    "    x3=Concatenate(axis=-1)([x2,x3])\n",
    "    \n",
    "    x4=Conv2D(filters=gr,kernel_size=3,strides=1,padding='same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(x3)\n",
    "    x4=BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(x4)\n",
    "    x4=LeakyReLU(alpha=0.2)(x4)\n",
    "\n",
    "    x4=Concatenate(axis=-1)([x3,x4])\n",
    "    \n",
    "    x5=Conv2D(filters=fil,kernel_size=3,strides=1,padding='same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(x4)\n",
    "    x5=Lambda(lambda x:x*beta)(x5)\n",
    "    xout=Add()([x5,x])\n",
    "    \n",
    "    return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resresden(x,fil,gr,betad,betar,gamma_init,trainable):\n",
    "    x1=resden(x,fil,gr,betad,gamma_init,trainable)\n",
    "    x2=resden(x1,fil,gr,betad,gamma_init,trainable)\n",
    "    x3=resden(x2,fil,gr,betad,gamma_init,trainable)\n",
    "    x3=Lambda(lambda x:x*betar)(x3)\n",
    "    xout=Add()([x3,x])\n",
    "    \n",
    "    return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inp_shape, trainable = True):\n",
    "    gamma_init = RandomNormal(1., 0.02)\n",
    "\n",
    "    fd=512\n",
    "    gr=32\n",
    "    nb=12\n",
    "    betad=0.2\n",
    "    betar=0.2\n",
    "\n",
    "    inp_real_imag = Input(inp_shape)\n",
    "    lay_128dn = Conv2D(64, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(inp_real_imag)\n",
    "\n",
    "    lay_128dn = LeakyReLU(alpha = 0.2)(lay_128dn)\n",
    "\n",
    "    lay_64dn = Conv2D(128, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_128dn)\n",
    "    lay_64dn = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_64dn)\n",
    "    lay_64dn = LeakyReLU(alpha = 0.2)(lay_64dn)\n",
    "\n",
    "    lay_32dn = Conv2D(256, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_64dn)\n",
    "    lay_32dn = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_32dn)\n",
    "    lay_32dn = LeakyReLU(alpha=0.2)(lay_32dn)\n",
    "\n",
    "    lay_16dn = Conv2D(512, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_32dn)\n",
    "    lay_16dn = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_16dn)\n",
    "    lay_16dn = LeakyReLU(alpha=0.2)(lay_16dn)  #16x16 \n",
    "\n",
    "    lay_8dn = Conv2D(512, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_16dn)\n",
    "    lay_8dn = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_8dn)\n",
    "    lay_8dn = LeakyReLU(alpha=0.2)(lay_8dn) #8x8\n",
    "\n",
    "\n",
    "    xc1=Conv2D(filters=fd,kernel_size=3,strides=1,padding='same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_8dn) #8x8\n",
    "    xrrd=xc1\n",
    "    for m in range(nb):\n",
    "        xrrd=resresden(xrrd,fd,gr,betad,betar,gamma_init,trainable)\n",
    "\n",
    "    xc2=Conv2D(filters=fd,kernel_size=3,strides=1,padding='same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(xrrd)\n",
    "    lay_8upc=Add()([xc1,xc2])\n",
    "\n",
    "    lay_16up = Conv2DTranspose(1024, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_8upc)\n",
    "    lay_16up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_16up)\n",
    "    lay_16up = Activation('relu')(lay_16up) #16x16\n",
    "\n",
    "    lay_16upc = Concatenate(axis = -1)([lay_16up,lay_16dn])\n",
    "\n",
    "    lay_32up = Conv2DTranspose(256, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_16upc) \n",
    "    lay_32up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_32up)\n",
    "    lay_32up = Activation('relu')(lay_32up) #32x32\n",
    "\n",
    "    lay_32upc = Concatenate(axis = -1)([lay_32up,lay_32dn])\n",
    "\n",
    "    lay_64up = Conv2DTranspose(128, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_32upc)\n",
    "    lay_64up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_64up)\n",
    "    lay_64up = Activation('relu')(lay_64up) #64x64\n",
    "\n",
    "    lay_64upc = Concatenate(axis = -1)([lay_64up,lay_64dn])\n",
    "\n",
    "    lay_128up = Conv2DTranspose(64, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_64upc)\n",
    "    lay_128up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_128up)\n",
    "    lay_128up = Activation('relu')(lay_128up) #128x128\n",
    "\n",
    "    lay_128upc = Concatenate(axis = -1)([lay_128up,lay_128dn])\n",
    "\n",
    "    lay_256up = Conv2DTranspose(64, (4,4), strides = (2,2), padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_128upc)\n",
    "    lay_256up = BatchNormalization(gamma_initializer = gamma_init, trainable = trainable)(lay_256up)\n",
    "    lay_256up = Activation('relu')(lay_256up) #256x256\n",
    "\n",
    "    out =  Conv2D(1, (1,1), strides = (1,1), activation = 'tanh', padding = 'same', use_bias = True, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(lay_256up)\n",
    "\n",
    "    model = Model(inputs = inp_real_imag, outputs = out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan_model(gen_model, dis_model, inp_shape):\n",
    "    dis_model.trainable = False\n",
    "    inp = Input(shape = inp_shape)\n",
    "    out_g = gen_model(inp)\n",
    "    out_dis = dis_model(out_g)\n",
    "    out_g1 = out_g\n",
    "    model = Model(inputs = inp, outputs = [out_dis, out_g, out_g1])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_par, d_par, gan_model, dataset_real, u_sampled_data,  n_epochs, n_batch, n_critic, clip_val, n_patch, f):\n",
    "    bat_per_epo = int(dataset_real.shape[0]/n_batch)\n",
    "    half_batch = int(n_batch/2)\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epo):\n",
    "            \n",
    "            # training the discriminator\n",
    "            for k in range(n_critic):\n",
    "                ix = np.random.randint(0, dataset_real.shape[0], half_batch)\n",
    "            \n",
    "                X_real = dataset_real[ix]\n",
    "                y_real = np.ones((half_batch,n_patch,n_patch,1))\n",
    "            \n",
    "                ix_1 =  np.random.randint(0, u_sampled_data.shape[0], half_batch)\n",
    "                X_fake  = g_par.predict(u_sampled_data[ix_1])\n",
    "                y_fake = -np.ones((half_batch,n_patch,n_patch,1))\n",
    "            \n",
    "                X, y = np.vstack((X_real, X_fake)), np.vstack((y_real,y_fake))\n",
    "                d_loss, accuracy = d_par.train_on_batch(X,y)\n",
    "            \n",
    "                for l in d_par.layers:\n",
    "                    weights=l.get_weights()\n",
    "                    weights=[np.clip(w, -clip_val,clip_val) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "            # training the generator\n",
    "            ix = np.random.randint(0, dataset_real.shape[0], n_batch)\n",
    "            X_r = dataset_real[ix]\n",
    "            X_gen_inp = u_sampled_data[ix]\n",
    "            y_gan = np.ones((n_batch,n_patch,n_patch,1))\n",
    "            \n",
    "            g_loss = gan_model.train_on_batch ([X_gen_inp], [y_gan, X_r, X_r])\n",
    "            f.write('>%d, %d/%d, d=%.3f, acc = %.3f,  w=%.3f,  mae=%.3f,  mssim=%.3f, g=%.3f' %(i+1, j+1, bat_per_epo, d_loss, accuracy, g_loss[1], g_loss[2], g_loss[3], g_loss[0]))\n",
    "            f.write('\\n')\n",
    "            print ('>%d, %d/%d, d=%.3f, acc = %.3f, g=%.3f' %(i+1, j+1, bat_per_epo, d_loss, accuracy, g_loss[0]))\n",
    "        filename = './gen_weights_a5_%04d.h5' % (i+1)\n",
    "        g_save = g_par.get_layer('model_3')\n",
    "        g_save.save_weights(filename)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters       \n",
    "n_epochs = 300\n",
    "n_batch = 32\n",
    "n_critic = 3\n",
    "clip_val = 0.05\n",
    "in_shape_gen = (256,256,2)\n",
    "in_shape_dis = (256,256,1)\n",
    "accel = 3\n",
    "\n",
    "d_model = discriminator(inp_shape = in_shape_dis, trainable = True)\n",
    "d_model.summary()\n",
    "# d_par = multi_gpu_model(d_model, gpus=4, cpu_relocation = True)  #for multi-gpu training\n",
    "d_par = d_model\n",
    "opt = Adam(lr = 0.0002, beta_1 = 0.5)\n",
    "d_par.compile(loss = wloss, optimizer = opt, metrics = [accw])\n",
    "\n",
    "g_model = generator(inp_shape = in_shape_gen , trainable = True)\n",
    "# g_par = multi_gpu_model(g_model, gpus=4, cpu_relocation = True)  #for multi-gpu training\n",
    "g_par = g_model\n",
    "g_par.summary()\n",
    "\n",
    "gan_model = define_gan_model(g_par, d_par, in_shape_gen)\n",
    "opt1 = Adam(lr = 0.0001, beta_1 = 0.5)\n",
    "gan_model.compile(loss = [wloss, 'mae', mssim], optimizer = opt1, loss_weights = [0.01, 20.0, 1.0]) #loss weights for generator training\n",
    "n_patch=d_model.output_shape[1]\n",
    "\n",
    "data_path='./training.pickle' # Ground truth\n",
    "usam_path='./training_usamp.pickle' # Zero-filled reconstructions\n",
    "dataset_real = pickle.load(open(data_path,'rb'))\n",
    "u_sampled_data = pickle.load(open(usam_path,'rb'))\n",
    "# dataset_real = np.expand_dims(dataset_real, axis = -1)\n",
    "# u_sampled_data = np.expand_dims(u_sampled_data, axis = -1)\n",
    "u_sampled_data_2c = np.concatenate((u_sampled_data.real, u_sampled_data.imag), axis = -1)\n",
    "\n",
    "f = open('./log_a5.txt', 'a') \n",
    "train(g_par, d_par, gan_model, dataset_real, u_sampled_data_2c, n_epochs, n_batch, n_critic, clip_val, n_patch, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e134e05457d34029b6460cd73bbf1ed73f339b5b6d98c95be70b69eba114fe95"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
